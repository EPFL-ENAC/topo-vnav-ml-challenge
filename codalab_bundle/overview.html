<html>
  <head>
    <title>overview</title>
    <meta http-equiv="content-type" content="text/html; charset=windows-1252">
  </head>
  <body>
    <h1><span style="font-family: Arial;"></span><span style="font-family: Arial;"><b>No
          GPS no problem! Democratising aerial navigation via robust and
          data-scalable computer vision.</b></span></h1>
    <p style="text-align: center;"><img src="https://lh3.googleusercontent.com/Jt_kgwhkawPlWMiB672org868ThWHEsmDfDVnOPEZ0JVUXFLXDIfJ0Os94nvVfLTWR_-y9r29_lSDoBi2MWQHBJRbbwOwcmleLSUZbd5iFsRICta9-1kVNZEWILObzjB1c9WzlaF4J8F_1X3qA"
        style="margin-left:-4.079118028534403px;margin-top:2.0006654285323604e-14px;"
        height="359" width="633.0791180285344"></p>
    <p><br>
      <span style="font-family: Arial;"></span></p>
    <p><span style="font-family: Arial;">Do you believe like us in pushing CV
        advances in the real world? Will you be a champion in proving CV
        advances benefits to the world of aviation? Then join us and earn:</span><span
        style="font-family: Arial;"><br>
      </span></p>
    <ul>
      <li><span style="font-family: Arial;">The prize: <b style="color: red;">10
            000 CHF</b> (8 000 CHF for 1st place and 2 000 CHF for second) based
          on the leaderboard ranking. Winners commit to share their solution
          (git and model explanation report) open source for the benefit of all.</span></li>
    </ul>
    <ul>
      <li><span style="font-family: Arial;">Respect: Being the leader in the
          real-world challenge of aerial navigation! Leaderboard open for all
          with no restrictions. </span><br>
        <span style="font-family: Arial;"></span></li>
    </ul>
    <span style="font-family: Arial;"><br>
      The value of research shines best when it touches everyday life! Today's
      scientific journals in Computer Vision (CV) provide exciting developments
      that promise a future in which CV systems can automise navigation, control
      of ground and aerial vehicles. Thus a lasting impact on our daily lives
      beyond the annals of academic journals. Traditionally, however, academic
      advancements are focusing on developing theoretical solutions optimised to
      perform over a given data domain, i.e a set of camera images collected at
      specific geographic location, trajectory path, camera type, weather,
      season, date, etc. Those algorithms can rarely be proven to generalise to
      real-world large-scale applications. Those would require an economically
      prohibitive amount of training data describing all possible situations and
      locations in time and space. This poses a challenge to democratisation and
      realisation of the autonomous/GPS-free promise. As a rising
      countermeasure, synthetic data and rendering engines have attempted to
      fill in the data hunger of the CV models (<a href="https://github.com/TOPO-EPFL/TOPO-DataGen">TOPO-DataGen</a>
      open source example for air nav domain). Two paths exist in bridging the
      gap between the real scarce data and abundant synthetic: <br>
      <br>
    </span>
    <ol>
      <li><span style="font-family: Arial;">Either making the synthetic data
          highly representative of reality or</span><span style="font-family: Arial;"></span></li>
      <li><span style="font-family: Arial;">Making models robust to sim-to-real
          (synthetic data to real data) gap</span></li>
    </ol>
    <p><br>
      <img src="https://lh3.googleusercontent.com/0tlCJnhJptiSDCRjto1s7qe3Zmdo2Y5M2RaNgHFA9CrBZG_fi_1idP8mqoqxWww6xThjsnt1c865e9gLW37aEmzmZ8JwEtsuCNV_aXAR69zoR_p6G9CPad9PUHoREROLi-AdEMWO5YE-EQL9YA"
        style="margin-left:0px;margin-top:0px;" height="342" width="527"></p>
    <address><i><span style="font-family: Arial;">Through the Looking-Glass,
          Alice Pushes Through the Mirror (between real and synthetic data) -
          drawn by John Tenniel, 1872</span></i></address>
    <span style="font-family: Arial;"></span><span style="font-family: Arial;"><br>
      We believe that the real-challenge of today's CV is to develop and
      validate a design philosophy for algorithms that can work robustly and in
      an economically/data scalable way. In other words:<br>
    </span>
    <ul>
      <li><span style="font-family: Arial;">Being able to train with limited or
          even no real/domain specific data (zero-shot). Those leveraging
          synthetic data generated by open source tools such as <a href="https://github.com/TOPO-EPFL/TOPO-DataGen">TOPO-DataGen</a>.</span><span
          style="font-family: Arial;"></span></li>
      <li><span style="font-family: Arial;">Capable of producing accurate output
          with trustworthy uncertainty statements that can capture accurately
          the prediction errors. [4]</span><br>
        <span style="font-family: Arial;"></span></li>
    </ul>
    <span style="font-family: Arial;"><br>
    </span>
    <p></p>
    <h2><span style="font-family: Arial;"><b>The challenge, should you choose to
          accept it.</b></span><span style="font-family: Arial;"></span></h2>
    <h2><span style="font-family: Arial;"></span></h2>
    <p><span style="font-family: Arial;">Today aerial autonomous systems for
        navigation and control are heavily dependent on the robustness of GNSS
        reception. Lack of thereof (terrain, weather, or adversarial spoofing)
        can lead to loss of autonomous system absolute orientation. This in the
        medium to long run (error will always accumulate in time with dead
        reckoning) can be unsafe for operation, especially on beyond line of
        sight missions. Despite significant progress in Computer Vision, most
        learning-based approaches target a single domain and require a dense
        database of geo-tagged images to function well. Or at least a
        calibration set of real images taken closely from the domain of [1,2]
        operation. Several industry attempts are made in the same direction,
        however, understandably without an official benchmark or validation.</span></p>
    <p><br>
      <span style="font-family: Arial;"></span></p>
    <p><img src="https://lh6.googleusercontent.com/DhS-jaBBTRywlgF9fjzA3lDhKPiQ8nAQ4ELrxySrMX9qObxiN889t1gvw7Cg5gnFfnTi6D9XrAQs0Fh7wsew3Mbv8y7lhHyTPMgVSO7oy6Jje9itEPen5OtTrAers_ALz-Szg6NDRKtskDzcSA"
        style="margin-left:0px;margin-top:0px;" height="160" width="668"><br>
      <br>
      <span style="font-family: Arial;"></span></p>
    <span style="font-family: Arial;"><br>
      We challenge you to prove your approach can work <b>robustly </b>and in
      an<b> economically/data scalable way</b>. To do this by proving you can
      compute accurate 6D camera poses with known uncertainty on our challenge
      validation dataset. To do this by having only access to:<br>
    </span><br>
  </body>
</html>
